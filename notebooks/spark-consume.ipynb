{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install FlagEmbedding\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "import requests\n",
    "\n",
    "\n",
    "# cache_dir = '/tmp/huggingface_cache'\n",
    "# os.makedirs(cache_dir, exist_ok=True)\n",
    "# os.chmod(cache_dir, 0o777)\n",
    "# os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "hf_token = \"hf_token\"\n",
    "api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\n",
    "headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaRawCSVReader\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.pyspark.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read raw data from Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"csv_stream\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "# Convert Kafka 'value' column to STRING\n",
    "kafka_string_df = kafka_df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "\n",
    "semantic_schema = (\"customer aged {Age} is a {Gender} with marital status {Marital_Status} is a {Occupation} with monthly income of {Monthly_Income} \"\n",
    "                   \"has completed {Educational_Qualifications} with family size of {Family_size} belongs to latitude {latitude} and \"\n",
    "                   \"longitude {longitude} with pincode {Pin_code} has given {Feedback} feedback\")\n",
    "\n",
    "\n",
    "def encode(final_sentence):\n",
    "    response = requests.post(api_url, headers=headers, json={\"inputs\": final_sentence, \"options\":{\"wait_for_model\":True}})\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def process_row(row):\n",
    "    csv = tuple(row.asDict().values())\n",
    "    csv_values_temp = tuple(csv[0].split(','))\n",
    "    \n",
    "    csv_values = tuple(value.strip('\"') for value in csv_values_temp)\n",
    "    \n",
    "    data = {\n",
    "        \"Age\": csv_values[0],\n",
    "        \"Gender\": csv_values[1],\n",
    "        \"Marital_Status\": csv_values[2],\n",
    "        \"Occupation\": csv_values[3],\n",
    "        \"Monthly_Income\": csv_values[4],\n",
    "        \"Educational_Qualifications\": csv_values[5],\n",
    "        \"Family_size\": csv_values[6],\n",
    "        \"latitude\": csv_values[7],\n",
    "        \"longitude\": csv_values[8],\n",
    "        \"Pin_code\": csv_values[9],\n",
    "        \"Feedback\": csv_values[10]\n",
    "    }\n",
    "\n",
    "    \n",
    "    final_sentence = semantic_schema.format(\n",
    "        Age=data[\"Age\"],\n",
    "        Gender=data[\"Gender\"],\n",
    "        Marital_Status=data[\"Marital_Status\"],\n",
    "        Occupation=data[\"Occupation\"],\n",
    "        Monthly_Income=data[\"Monthly_Income\"],\n",
    "        Educational_Qualifications=data[\"Educational_Qualifications\"],\n",
    "        Family_size=data[\"Family_size\"],\n",
    "        latitude=data[\"latitude\"],\n",
    "        longitude=data[\"longitude\"],\n",
    "        Pin_code=data[\"Pin_code\"],\n",
    "        Feedback=data[\"Feedback\"]\n",
    "    )\n",
    "\n",
    "    # print(\"final_sentence\",final_sentence)\n",
    "    embedded_sentence = encode(final_sentence)\n",
    "    print(\"embedded_sentence\",embedded_sentence)    \n",
    "    \n",
    "\n",
    "query = kafka_string_df.writeStream.foreach(process_row).start()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 22.3 from /opt/conda/lib/python3.10/site-packages/pip (python 3.10)\n",
      "Collecting setuptools<70\n",
      "  Downloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 72.0.0\n",
      "    Uninstalling setuptools-72.0.0:\n",
      "      Removing file or directory /opt/conda/lib/python3.10/site-packages/_distutils_hack/\n",
      "      Removing file or directory /opt/conda/lib/python3.10/site-packages/distutils-precedence.pth\n",
      "      Removing file or directory /opt/conda/lib/python3.10/site-packages/pkg_resources/\n",
      "      Removing file or directory /opt/conda/lib/python3.10/site-packages/setuptools-72.0.0.dist-info/\n",
      "      Removing file or directory /opt/conda/lib/python3.10/site-packages/setuptools/\n",
      "      Successfully uninstalled setuptools-72.0.0\n",
      "Successfully installed setuptools-69.5.1\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall backports\n",
    "# !pip install backports\n",
    "!pip install --force-reinstall -v \"setuptools<70\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "['default', 'my_database', 'my_database2']\n"
     ]
    }
   ],
   "source": [
    "# !pip install pymilvus==2.4.4\n",
    "\n",
    "from pymilvus import connections, db\n",
    "\n",
    "conn = connections.connect(host=\"standalone\", port=19530)\n",
    "\n",
    "print(conn)\n",
    "\n",
    "# database = db.create_database(\"my_database2\")\n",
    "\n",
    "print(db.list_database())\n",
    "\n",
    "# print(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Define the semantic schema\n",
    "# semantic_schema = (\"customer aged {Age} is a {Gender} with marital status {Marital_Status} is a {Occupation} with {Monthly_Income} \"\n",
    "#                    \"has completed {Educational_Qualifications} with family size of {Family_size} belongs to latitude {latitude} and \"\n",
    "#                    \"longitude {longitude} with pincode {Pin_code} has given {Feedback} feedback\")\n",
    "\n",
    "# # Step 2: Define the CSV values as a tuple\n",
    "# csv_values = (\"26\", \"Male\", \"Married\", \"Employee\", \"More than 50000\", \"Graduate\", \"4\", \"12.9048\", \"77.6821\", \"560036\", \"Positive\")\n",
    "\n",
    "# # Step 3: Create a dictionary to map placeholder names to CSV values\n",
    "# data = {\n",
    "#     \"Age\": csv_values[0],\n",
    "#     \"Gender\": csv_values[1],\n",
    "#     \"Marital_Status\": csv_values[2],\n",
    "#     \"Occupation\": csv_values[3],\n",
    "#     \"Monthly_Income\": csv_values[4],\n",
    "#     \"Educational_Qualifications\": csv_values[5],\n",
    "#     \"Family_size\": csv_values[6],\n",
    "#     \"latitude\": csv_values[7],\n",
    "#     \"longitude\": csv_values[8],\n",
    "#     \"Pin_code\": csv_values[9],\n",
    "#     \"Feedback\": csv_values[10]\n",
    "# }\n",
    "\n",
    "# # Step 4: Use the .format() method to populate the template\n",
    "# final_sentence = semantic_schema.format(\n",
    "#     Age=data[\"Age\"],\n",
    "#     Gender=data[\"Gender\"],\n",
    "#     Marital_Status=data[\"Marital_Status\"],\n",
    "#     Occupation=data[\"Occupation\"],\n",
    "#     Monthly_Income=data[\"Monthly_Income\"],\n",
    "#     Educational_Qualifications=data[\"Educational_Qualifications\"],\n",
    "#     Family_size=data[\"Family_size\"],\n",
    "#     latitude=data[\"latitude\"],\n",
    "#     longitude=data[\"longitude\"],\n",
    "#     Pin_code=data[\"Pin_code\"],\n",
    "#     Feedback=data[\"Feedback\"]\n",
    "# )\n",
    "\n",
    "# print(final_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install FlagEmbedding\n",
    "\n",
    "# import os\n",
    "# cache_dir = '/tmp/huggingface_cache'\n",
    "# os.makedirs(cache_dir, exist_ok=True)\n",
    "# os.chmod(cache_dir, 0o777)\n",
    "# os.environ['TRANSFORMERS_CACHE'] = cache_dir\n",
    "\n",
    "# from FlagEmbedding import BGEM3FlagModel\n",
    "# model = BGEM3FlagModel('BAAI/bge-m3',  \n",
    "#                        use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "\n",
    "# def encode(final_sentence):\n",
    "#     sentence_embeddings = model.encode(final_sentence, \n",
    "#                             batch_size=12, \n",
    "#                             max_length=8192, # If you don't need such a long length, you can set a smaller value to speed up the encoding process.\n",
    "#                             )['dense_vecs']\n",
    "#     print(sentence_embeddings)\n",
    "#     return sentence_embeddings\n",
    "\n",
    "# encode(final_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
